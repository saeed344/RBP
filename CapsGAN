{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":8662436,"datasetId":5119593,"databundleVersionId":8813312}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2024-06-23T20:52:31.389464Z","iopub.execute_input":"2024-06-23T20:52:31.389787Z","iopub.status.idle":"2024-06-23T20:52:32.346721Z","shell.execute_reply.started":"2024-06-23T20:52:31.389761Z","shell.execute_reply":"2024-06-23T20:52:32.345723Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/qsar-classification/4_feature_selection_correlation.py\n/kaggle/input/qsar-classification/5_Feature_Selection_RFE_RF.py\n/kaggle/input/qsar-classification/independent_test1.py\n/kaggle/input/qsar-classification/lebels.csv\n/kaggle/input/qsar-classification/training.py\n/kaggle/input/qsar-classification/TwoD_clean.csv\n/kaggle/input/qsar-classification/1D.csv\n/kaggle/input/qsar-classification/2D/EState.csv\n/kaggle/input/qsar-classification/2D/Pubchem.csv\n/kaggle/input/qsar-classification/2D/RDK5.csv\n/kaggle/input/qsar-classification/2D/FP4C.csv\n/kaggle/input/qsar-classification/2D/AP2D.csv\n/kaggle/input/qsar-classification/2D/MACCS.csv\n/kaggle/input/qsar-classification/2D/CDKExt.csv\n/kaggle/input/qsar-classification/2D/KRC.csv\n/kaggle/input/qsar-classification/2D/KR.xlsx\n/kaggle/input/qsar-classification/2D/CDKGraph.csv\n/kaggle/input/qsar-classification/2D/CDK.csv\n/kaggle/input/qsar-classification/2D/KR.csv\n/kaggle/input/qsar-classification/2D/Morgan.csv\n/kaggle/input/qsar-classification/2D/Label.csv\n/kaggle/input/qsar-classification/2D/AP2DC.csv\n/kaggle/input/qsar-classification/2D/FP4.csv\n/kaggle/input/qsar-classification/Mol2Vec/R2D100.csv\n/kaggle/input/qsar-classification/Mol2Vec/R3D100.csv\n/kaggle/input/qsar-classification/Mol2Vec/R5D300.xlsx\n/kaggle/input/qsar-classification/Mol2Vec/R8D300.csv\n/kaggle/input/qsar-classification/Mol2Vec/R7D300.csv\n/kaggle/input/qsar-classification/Mol2Vec/R5D300.csv\n/kaggle/input/qsar-classification/Mol2Vec/R7D100.csv\n/kaggle/input/qsar-classification/Mol2Vec/R4D100.csv\n/kaggle/input/qsar-classification/Mol2Vec/R6D100.csv\n/kaggle/input/qsar-classification/Mol2Vec/R5D100.xlsx\n/kaggle/input/qsar-classification/Mol2Vec/R9D300.csv\n/kaggle/input/qsar-classification/Mol2Vec/R4D300.csv\n/kaggle/input/qsar-classification/Mol2Vec/R0D100.csv\n/kaggle/input/qsar-classification/Mol2Vec/R9D100.csv\n/kaggle/input/qsar-classification/Mol2Vec/R2D300.csv\n/kaggle/input/qsar-classification/Mol2Vec/R6D300.csv\n/kaggle/input/qsar-classification/Mol2Vec/R1D300.csv\n/kaggle/input/qsar-classification/Mol2Vec/R8D100.csv\n/kaggle/input/qsar-classification/Mol2Vec/R1D100.csv\n/kaggle/input/qsar-classification/Mol2Vec/R5D100.csv\n/kaggle/input/qsar-classification/Mol2Vec/R3D300.csv\n/kaggle/input/qsar-classification/Mol2Vec/R0D300.csv\n/kaggle/input/qsar-classification/3D/AP3D.csv\n/kaggle/input/qsar-classification/3D/3DMorse.csv\n/kaggle/input/qsar-classification/3D/RDK3D.csv\n/kaggle/input/qsar-classification/3D/EC3P.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import scale,StandardScaler\n\nimport pandas as pd\nimport numpy as np\n\npath1 = '/kaggle/input/qsar-classification/2D/'\npath4 = '/kaggle/input/qsar-classification/Mol2Vec/'\n# Read data\n\n# Read data\nAP2D = pd.read_csv(path1+'AP2D.csv').iloc[:, 1:]\n\nAP2DC = pd.read_csv(path1+'AP2DC.csv').iloc[:, 1:]\n\nCDK = pd.read_csv(path1+'CDK.csv').iloc[:, 1:]\n\nCDKExt = pd.read_csv(path1+'CDKExt.csv').iloc[:, 1:]\n\nCDKGraph = pd.read_csv(path1+'CDKGraph.csv').iloc[:, 1:]\n\nEState = pd.read_csv(path1+'EState.csv').iloc[:, 1:]\n\nFP4 = pd.read_csv(path1+'FP4.csv').iloc[:, 1:]\n\nFP4C = pd.read_csv(path1+'FP4C.csv').iloc[:, 1:]\n\nKR = pd.read_csv(path1+'KR.csv').iloc[:, 1:]\n\nKRC = pd.read_csv(path1+'KRC.csv').iloc[:, 1:]\n\nMACCS = pd.read_csv(path1+'MACCS.csv').iloc[:, 1:]\n\nMorgan = pd.read_csv(path1+'Morgan.csv').iloc[:, 1:]\n\nPubchem = pd.read_csv(path1+'Pubchem.csv').iloc[:, 1:]\n\nRDK5 = pd.read_csv(path1+'RDK5.csv').iloc[:, 1:]\n\n\nR0D100 = pd.read_csv(path4+'R0D100.csv').iloc[:, 1:]\nR0D300 = pd.read_csv(path4+'R0D300.csv').iloc[:, 1:]\nR1D100 = pd.read_csv(path4+'R1D100.csv').iloc[:, 1:]\nR1D300 = pd.read_csv(path4+'R1D300.csv').iloc[:, 1:]\nR2D100 = pd.read_csv(path4+'R2D100.csv').iloc[:, 1:]\nR2D300 = pd.read_csv(path4+'R2D300.csv').iloc[:, 1:]\nR3D100 = pd.read_csv(path4+'R3D100.csv').iloc[:, 1:]\nR3D300 = pd.read_csv(path4+'R3D300.csv').iloc[:, 1:]\nR4D100 = pd.read_csv(path4+'R4D100.csv').iloc[:, 1:]\nR4D300 = pd.read_csv(path4+'R4D300.csv').iloc[:, 1:]\nR5D100 = pd.read_csv(path4+'R5D100.csv').iloc[:, 1:]\nR5D300 = pd.read_csv(path4+'R5D300.csv').iloc[:, 1:]\nR6D100 = pd.read_csv(path4+'R6D100.csv').iloc[:, 1:]\nR6D300 = pd.read_csv(path4+'R6D300.csv').iloc[:, 1:]\nR7D100 = pd.read_csv(path4+'R7D100.csv').iloc[:, 1:]\nR7D300 = pd.read_csv(path4+'R7D300.csv').iloc[:, 1:]\nR8D100 = pd.read_csv(path4+'R8D100.csv').iloc[:, 1:]\nR8D300 = pd.read_csv(path4+'R8D300.csv').iloc[:, 1:]\nR9D100 = pd.read_csv(path4+'R9D100.csv').iloc[:, 1:]\nR9D300 = pd.read_csv(path4+'R9D300.csv').iloc[:, 1:]\n\n\n# two_D_feat = np.column_stack((AP2D, AP2DC, CDK, CDKExt, CDKGraph, EState, FP4, FP4C, KR,KRC, MACCS, \n#                               Morgan, PubchemRDK5))\n\ntwo_D_feat = np.column_stack((CDK, CDKExt, KR,KRC, Pubchem,RDK5))\n\nMol2Vec_feat = np.column_stack((R0D100,R0D300,R1D100,R1D100,R1D300,R2D100,R2D300,R3D100,R3D300,\n                                R4D100,R4D300,R5D100,R5D300,R6D100,R6D300,R7D100,R7D300,R8D100,\n                                R8D300,R9D100,R9D300))\ndataset = np.column_stack((two_D_feat, Mol2Vec_feat))\ndata_train=np.array(two_D_feat)\ntwoD_Mol2Vec_feat=data_train[:,:]\nshu=scale(twoD_Mol2Vec_feat)\n[m1,n1]=np.shape(Mol2Vec_feat)\nX=shu\n# label1=np.ones((int(598),1))#Value can be changed\n# label2=np.zeros((int(508 ),1))\n# label1=np.ones((598,1))#Value can be changed\n# label2=np.zeros((508,1))\n# label=np.append(label1,label2)\n# Assuming labels have already been read\nlabels = pd.read_csv('/kaggle/input/qsar-classification/2D/Label.csv')\n\n# Convert 'active' to 1 and 'inactive' to 0\nlabels['Activity'] = labels['Activity'].map({'active': 1, 'inactive': 0})\ny = labels['Activity'].values\n\n\ndataset = np.column_stack((two_D_feat, Mol2Vec_feat))\ndata_train=np.array(dataset)\ndata_train=data_train[:,:]\n[m1,n1]=np.shape(data_train)\n# label1=np.ones((int(598),1))#Value can be changed\n# # label2=np.zeros((int(508 ),1))\n# label1=np.ones((598,1))#Value can be changed\n# label2=np.zeros((508,1))\n# label=np.append(label1,label2)\n# Standardize the data\nscaler = StandardScaler()\nshu = scaler.fit_transform(data_train)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T20:52:32.348301Z","iopub.execute_input":"2024-06-23T20:52:32.348702Z","iopub.status.idle":"2024-06-23T20:52:40.673003Z","shell.execute_reply.started":"2024-06-23T20:52:32.348669Z","shell.execute_reply":"2024-06-23T20:52:40.671976Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_selection import mutual_info_regression\n\n\n\n# Split the dataset into features and target\nX = shu\n\n# Apply Information Gain\nig = mutual_info_regression(X, y)\n\n# Create a dictionary of feature importance scores using index-based feature names\nfeature_scores = {f\"Feature_{i}\": ig[i] for i in range(len(ig))}\n\n# Sort the features by importance score in descending order\nsorted_features = sorted(feature_scores.items(), key=lambda x: x[1], reverse=True)\n\n# Select the top 500 features\ntop_500_features = [int(feature.split('_')[1]) for feature, score in sorted_features[:500]]\n\n# Create a new dataset with only the top 500 features\nX_top_500 = X[:, top_500_features]\n\n# # Print the feature importance scores and the sorted features\n# for feature, score in sorted_features:\n#     print(\"Feature:\", feature, \"Score:\", score)\n\n# Plot a horizontal bar chart of the feature importance scores\nfig, ax = plt.subplots()\ny_pos = np.arange(len(sorted_features))\nax.barh(y_pos, [score for feature, score in sorted_features], align=\"center\")\nax.set_yticks(y_pos)\nax.set_yticklabels([feature for feature, score in sorted_features])\nax.invert_yaxis()  # Labels read top-to-bottom\nax.set_xlabel(\"Importance Score\")\nax.set_title(\"Feature Importance Scores (Information Gain)\")\n\n# Add importance scores as labels on the horizontal bar chart\nfor i, v in enumerate([score for feature, score in sorted_features]):\n    ax.text(v + 0.01, i, str(round(v, 3)), color=\"black\", fontweight=\"bold\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-23T20:52:48.512898Z","iopub.execute_input":"2024-06-23T20:52:48.513707Z","iopub.status.idle":"2024-06-23T20:55:38.744718Z","shell.execute_reply.started":"2024-06-23T20:52:48.513673Z","shell.execute_reply":"2024-06-23T20:55:38.743114Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"\nKeyboardInterrupt\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"!pip install torch==1.13.0\n!install tensorflow==2.8.0\n!pip install tensorflow==2.8.0","metadata":{"execution":{"iopub.status.busy":"2024-06-23T20:55:38.747770Z","iopub.status.idle":"2024-06-23T20:55:38.748110Z","shell.execute_reply.started":"2024-06-23T20:55:38.747946Z","shell.execute_reply":"2024-06-23T20:55:38.747960Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport sys\nimport math\nimport numpy as np\nimport pandas as pd\nimport statistics\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import scale\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import initializers, regularizers, constraints\nfrom tensorflow.keras.activations import sigmoid\nfrom tensorflow.keras.layers import Input, Dense, Layer, Reshape, Flatten\nfrom tensorflow.keras.layers import multiply, Add, Permute\nfrom tensorflow.keras.layers import Dropout, Lambda, Concatenate, Multiply\nfrom tensorflow.keras.layers import BatchNormalization, Activation\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D\nfrom tensorflow.keras.layers import LeakyReLU\nfrom tensorflow.keras.layers import UpSampling2D, Conv2D\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.optimizers import adam_v2\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom sklearn.decomposition import PCA\n\nX = X_top_500\ny=y\n\n\n# Split the data into training (80%) and testing (20%) sets\nX_train, X_test, y_train, y_test = train_test_split(X, label, test_size=0.2, random_state=42)\n\n\n# Assuming X_train_whole = scale(X_train_whole)\n[sample_num, input_dimwx] = np.shape(X_train)\nX = X_train\ny = y_train\nxt = X_test\nyt = y_test\n\n\ndef get_shuffle(data, label):\n    index = np.arange(len(label))\n    np.random.shuffle(index)\n    return data[index], label[index]\n\nX, y = get_shuffle(X, y)\n\ndef scale_mean_var(input_arr, axis=0):\n    mean_ = np.mean(input_arr, axis=0)\n    scale_ = np.std(input_arr, axis=0)\n    output_arr = input_arr - mean_\n    mean_1 = output_arr.mean(axis=0)\n    if not np.allclose(mean_1, 0):\n        output_arr -= mean_1\n    scale_[scale_ == 0.0] = 1.0\n    output_arr /= scale_\n    mean_2 = output_arr.mean(axis=0)\n    if not np.allclose(mean_2, 0):\n        output_arr -= mean_2\n    return output_arr\n\n########################################################### Def Cbam\ndef channel_attention(input_feature, ratio=8):\n\tchannel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n\tchannel = input_feature.shape[channel_axis]\n\tshared_layer_one = Dense(channel//ratio,\n\t\t\t\t\t\t\t kernel_initializer='he_normal',\n\t\t\t\t\t\t\t activation = 'relu',\n\t\t\t\t\t\t\t use_bias=True,\n\t\t\t\t\t\t\t bias_initializer='zeros')\n\tshared_layer_two = Dense(channel,\n\t\t\t\t\t\t\t kernel_initializer='he_normal',\n\t\t\t\t\t\t\t use_bias=True,\n\t\t\t\t\t\t\t bias_initializer='zeros')\n\tavg_pool = GlobalAveragePooling2D()(input_feature)\n\tavg_pool = Reshape((1,1,channel))(avg_pool)\n\tassert avg_pool.shape[1:] == (1,1,channel)\n\tavg_pool = shared_layer_one(avg_pool)\n\tassert avg_pool.shape[1:] == (1,1,channel//ratio)\n\tavg_pool = shared_layer_two(avg_pool)\n\tassert avg_pool.shape[1:] == (1,1,channel)\n\tmax_pool = GlobalMaxPooling2D()(input_feature)\n\tmax_pool = Reshape((1,1,channel))(max_pool)\n\tassert max_pool.shape[1:] == (1,1,channel)\n\tmax_pool = shared_layer_one(max_pool)\n\tassert max_pool.shape[1:] == (1,1,channel//ratio)\n\tmax_pool = shared_layer_two(max_pool)\n\tassert max_pool.shape[1:] == (1,1,channel)\n\tcbam_feature = Add()([avg_pool,max_pool])\n\tcbam_feature = Activation('hard_sigmoid')(cbam_feature)\n\tif K.image_data_format() == \"channels_first\":\n\t\tcbam_feature = Permute((3, 1, 2))(cbam_feature)\n\treturn multiply([input_feature, cbam_feature])\n\n\ndef spatial_attention(input_feature):\n\tkernel_size = 7\n\tif K.image_data_format() == \"channels_first\":\n\t\tchannel = input_feature.shape[1]\n\t\tcbam_feature = Permute((2,3,1))(input_feature)\n\telse:\n\t\tchannel = input_feature.shape[-1]\n\t\tcbam_feature = input_feature\n\tavg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n\tassert avg_pool.shape[-1] == 1\n\tmax_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n\tassert max_pool.shape[-1] == 1\n\tconcat = Concatenate(axis=3)([avg_pool, max_pool])\n\tassert concat.shape[-1] == 2\n\tcbam_feature = Conv2D(filters = 1,\n\t\t\t\t\tkernel_size=kernel_size,\n\t\t\t\t\tactivation = 'hard_sigmoid',\n\t\t\t\t\tstrides=1,\n\t\t\t\t\tpadding='same',\n\t\t\t\t\tkernel_initializer='he_normal',\n\t\t\t\t\tuse_bias=False)(concat)\n\tassert cbam_feature.shape[-1] == 1\n\tif K.image_data_format() == \"channels_first\":\n\t\tcbam_feature = Permute((3, 1, 2))(cbam_feature)\n\treturn multiply([input_feature, cbam_feature])\n\n\ndef cbam_block(cbam_feature,ratio=8):\n\tcbam_feature = channel_attention(cbam_feature, ratio)\n\tcbam_feature = spatial_attention(cbam_feature, )\n\treturn cbam_feature\n\n\n############################################## Def discriminator and generator\ndef squash(vectors, axis=-1):\n    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm + K.epsilon())\n    return scale * vectors\n\ndef build_discriminator():\n    img = Input(shape=(1,input_dimwx,1))\n    x = Conv2D(filters=64, kernel_size=(1,9), strides=2, padding='valid', name='conv1')(img)\n    x = LeakyReLU()(x)\n    x = BatchNormalization(momentum=0.8)(x)\n\n    x = Conv2D(filters=32, kernel_size=(1,9), strides=2, padding='valid', name='conv1')(img)\n    x = LeakyReLU()(x)\n    x = BatchNormalization(momentum=0.8)(x)\n\n    \"\"\"\n    NOTE: Capsule architecture starts from here.\n    \"\"\"\n    ##### primarycaps coming first #####\n    x = Conv2D(filters=32, kernel_size=(1,3), strides=2, padding='valid', name='primarycap_conv2')(x)\n    [aa,bb,cc,dd] = x.shape\n    numx = int(cc)\n    x = Reshape(target_shape=[-1, numx], name='primarycap_reshape')(x)\n    x = Lambda(squash, name='primarycap_squash')(x)\n    x = BatchNormalization(momentum=0.8)(x)\n\n    ##### digitcaps are here #####\n    x = Flatten()(x)\n    uhat = Dense(128, kernel_initializer='he_normal', bias_initializer='zeros', name='uhat_digitcaps')(x)\n    c = Activation('softmax', name='softmax_digitcaps1')(uhat) # softmax will make sure that each weight c_ij is a non-negative number and their sum equals to one\n    c = Dense(128)(c) # compute s_j\n    x = Multiply()([uhat, c])\n    \"\"\"\n    NOTE: Squashing the capsule outputs creates severe blurry artifacts, thus we replace it with Leaky ReLu.\n    \"\"\"\n    s_j = LeakyReLU()(x)\n    ##### we will repeat the routing part 2 more times (num_routing=3) to unfold the loop\n    c = Activation('softmax', name='softmax_digitcaps2')(s_j) # softmax will make sure that each weight c_ij is a non-negative number and their sum equals to one\n    c = Dense(128)(c) # compute s_j\n    x = Multiply()([uhat, c])\n    s_j = LeakyReLU()(x)\n\n    c = Activation('softmax', name='softmax_digitcaps3')(s_j) # softmax will make sure that each weight c_ij is a non-negative number and their sum equals to one\n    c = Dense(128)(c) # compute s_j\n    x = Multiply()([uhat, c])\n    s_j = LeakyReLU()(x)\n\n    c = Activation('softmax', name='softmax_digitcaps4')(s_j) # softmax will make sure that each weight c_ij is a non-negative number and their sum equals to one\n    c = Dense(128)(c) # compute s_j\n    x = Multiply()([uhat, c])\n    s_j = LeakyReLU()(x)\n    # ##### preparition for cbam_block\n    s_j = Reshape((-1,128,1))(s_j)\n    inputs = s_j\n    residual = Conv2D(filters=64, kernel_size=(1,1), strides=1, padding='same', name='convxxx')(inputs)\n    residual = BatchNormalization(momentum=0.8)(residual)\n\n    cbam = cbam_block(residual)\n    # cbam = channel_attention(residual)\n    # cbam = spatial_attention(residual)\n\n    cbam = Reshape((-1,))(cbam)\n    pred = Dense(2, activation='sigmoid')(cbam)\n\n    # cbam = Reshape((-1,))(s_j)\n    # pred = Dense(2, activation='sigmoid')(cbam)\n\n\n    return Model(img, pred)\n\ndiscriminator = build_discriminator()\ndiscriminator.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['binary_accuracy'])\n\n\n# generator structure\ndef build_generator():\n    \"\"\"\n    Generator follows the DCGAN architecture and creates generated image representations through learning.\n    \"\"\"\n    noise_shape =(input_dimwx,)\n    x_noise = Input(shape=noise_shape)\n    # we apply different kernel sizes in order to match the original image size\n    x = Dense(64 * 1 * input_dimwx, activation=\"relu\")(x_noise)\n    x = Reshape((1, input_dimwx, 64))(x)\n    x = BatchNormalization(momentum=0.2)(x)\n    x = UpSampling2D()(x)\n    [aa1,bb1,cc1,dd1] = x.shape\n    numx1 = int(cc1//4)\n    x = Conv2D(32, kernel_size=(2,numx1), padding=\"valid\")(x)\n    x = Activation(\"relu\")(x)\n    x = BatchNormalization(momentum=0.2)(x)\n    [aa2,bb2,cc2,dd2] = x.shape\n    #### x = UpSampling2D()(x)\n    numx2 = int(1+cc2-input_dimwx)\n    x = Conv2D(16, kernel_size=(1,numx2), padding=\"valid\")(x)\n    x = Activation(\"relu\")(x)\n    x = BatchNormalization(momentum=0.2)(x)\n    x = Conv2D(1, kernel_size=3, padding=\"same\")(x)\n    gen_out = Activation(\"tanh\")(x)\n\n    return Model(x_noise, gen_out)\n\ngenerator = build_generator()\ngenerator.compile(loss='binary_crossentropy', optimizer=adam_v2.Adam(0.002, 0.8), metrics=['binary_accuracy'])\n\n# feeding noise to generator\nz = Input(shape=(input_dimwx,))\nimg = generator(z)\n# for the combined model we will only train the generator\ndiscriminator.trainable = False\n# try to discriminate generated images\nvalid = discriminator(img)\n# the combined model (stacked generator and discriminator) takes\n# noise as input => generates images => determines validity\ncombined = Model(z, valid)\ncombined.compile(loss='binary_crossentropy', optimizer=adam_v2.Adam(0.002, 0.8), metrics=['binary_accuracy'])\n#########################################################\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n\n\ndef categorical_probas_to_classes(p):\n    return np.argmax(p, axis=1)\n\ndef to_categorical(y, nb_classes=None):\n    y = np.array(y, dtype='int')\n    if not nb_classes:\n        nb_classes = np.max(y) + 1\n    Y = np.zeros((len(y), nb_classes))\n    for i in range(len(y)):\n        Y[i, y[i]] = 1\n    return Y\n\nBACC_collecton = []\nSn_collecton = []\nSp_collecton = []\nMCC_collecton = []\nAUC_collecton = []\nAP = []\n\nmean_recall = np.linspace(0, 1, 100)\nall_precision = []\nbase_fpr = np.linspace(0, 1, 100)\nmean_tpr = 0.0\ninterp_tpr_collection = []\n\n# Define the directory to save the models\nsave_dir = '/kaggle/working/models/'\n\nskf = StratifiedKFold(n_splits=10)\nfor fold, (train, test) in enumerate(skf.split(X, y)):\n    X_train, X_valid, y_train, y_valid = X[train], X[test], y[train], y[test]\n    y_train = to_categorical(y_train)\n\n    # Reinitialize and compile models for each fold\n    discriminator = build_discriminator()\n    discriminator.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['binary_accuracy'])\n\n    generator = build_generator()\n    generator.compile(loss='binary_crossentropy', optimizer=adam_v2.Adam(0.002, 0.8), metrics=['binary_accuracy'])\n\n    z = Input(shape=(input_dimwx,))\n    img = generator(z)\n    discriminator.trainable = False\n    valid = discriminator(img)\n    combined = Model(z, valid)\n    combined.compile(loss='binary_crossentropy', optimizer=adam_v2.Adam(0.002, 0.8), metrics=['binary_accuracy'])\n\n    # Define checkpoint callback to save the best model\n    model_checkpoint = ModelCheckpoint(os.path.join(save_dir, f'model_fold_{fold+1}.h5'),\n                                       save_best_only=True,\n                                       monitor='val_loss',\n                                       mode='min')\n\n#     hist = combined.fit(X_train, y_train, batch_size=128, epochs=30, validation_data=(X_valid, to_categorical(y_valid)), callbacks=[model_checkpoint])\n    \n    \n\n    hist = combined.fit(X_train, y_train, batch_size=128, epochs=30, validation_data=(X_valid, to_categorical(y_valid)), \n                    callbacks=[model_checkpoint, early_stopping, reduce_lr])\n    # Load best model\n    # combined.load_weights(f'/content/drive/MyDrive/QSAR/models/model_fold_{fold+1}.h5')\n\n    y_score = combined.predict(X_valid)\n    y_class = categorical_probas_to_classes(y_score)\n\n    TP, FP, FN, TN = confusion_matrix(y_valid, y_class).ravel()\n    Sn_collecton.append(TP/(TP+FN))\n    Sp_collecton.append(TN/(TN+FP))\n    MCC = (TP*TN - FP*FN) / math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))\n    MCC_collecton.append(MCC)\n    BACC_collecton.append(0.5*TP/(TP+FN) + 0.5*TN/(TN+FP))\n\n    fpr, tpr, _ = roc_curve(y_valid, y_score[:, 1])\n    interp_tpr = np.interp(base_fpr, fpr, tpr)\n    interp_tpr[0] = 0.0\n    interp_tpr_collection.append(interp_tpr)\n    auc_roc = auc(fpr, tpr)\n    AUC_collecton.append(auc_roc)\n\n    precision, recall, _ = precision_recall_curve(y_valid, y_score[:, 1])\n    average_precision = average_precision_score(y_valid, y_score[:, 1])\n    recall = np.flipud(recall)\n    precision = np.flipud(precision)\n\n    mean_precision = np.interp(mean_recall, recall, precision)\n    all_precision.append(mean_precision)\n    AP.append(average_precision)\n    # Save the trained models\n    generator.save(os.path.join(save_dir, f'generator_model_fold_{fold+1}.h5'))\n    discriminator.save(os.path.join(save_dir, f'discriminator_model_fold_{fold+1}.h5'))\n\nprint(f'Balanced Accuracy: {np.mean(BACC_collecton):.3f} ± {np.std(BACC_collecton):.3f}')\nprint(f'Sensitivity: {np.mean(Sn_collecton):.3f} ± {np.std(Sn_collecton):.3f}')\nprint(f'Specificity: {np.mean(Sp_collecton):.3f} ± {np.std(Sp_collecton):.3f}')\nprint(f'MCC: {np.mean(MCC_collecton):.3f} ± {np.std(MCC_collecton):.3f}')\nprint(f'AUC: {np.mean(AUC_collecton):.3f} ± {np.std(AUC_collecton):.3f}')\nprint(f'Average Precision: {np.mean(AP):.3f} ± {np.std(AP):.3f}')\n\nmean_tpr = np.mean(interp_tpr_collection, axis=0)\nmean_tpr[-1] = 1.0\n\nmean_precision = np.mean(all_precision, axis=0)\n\nnp.savez('ROC_curve.npz', fpr=base_fpr, tpr=mean_tpr, roc_auc=AUC_collecton)\nnp.savez('PR_curve.npz', recall=mean_recall, precision=mean_precision, average_precision=AP)\n\nplt.figure()\nplt.plot(base_fpr, mean_tpr, color='darkorange', lw=2, label=f'ROC curve (area = {np.mean(AUC_collecton):.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([-0.05, 1.05])\nplt.ylim([-0.05, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('10-fold Cross-Validation ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-29T17:12:53.937043Z","iopub.execute_input":"2024-05-29T17:12:53.937424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport sys\nimport math\nimport numpy as np\nimport pandas as pd\nimport statistics\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import scale\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import initializers, regularizers, constraints\nfrom tensorflow.keras.activations import sigmoid\nfrom tensorflow.keras.layers import Input, Dense, Layer, Reshape, Flatten\nfrom tensorflow.keras.layers import multiply, Add, Permute\nfrom tensorflow.keras.layers import Dropout, Lambda, Concatenate, Multiply\nfrom tensorflow.keras.layers import BatchNormalization, Activation\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D\nfrom tensorflow.keras.layers import LeakyReLU\nfrom tensorflow.keras.layers import UpSampling2D, Conv2D\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.models import load_model\nfrom keras.optimizers import adam_v2\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\nBiLSTM=pd.read_csv(r'/kaggle/input/qsar-model/all_BiLSTM_train_2D.csv',header=None)\n\nAttention = pd.read_csv(r'/kaggle/input/qsar-model/all_atten_train_2D.csv',header=None)\n\ndataset = np.column_stack((BiLSTM, Attention))\n\ndata_train=np.array(dataset)\ndata_train=data_train[:1106,:]\n[m1,n1]=np.shape(data_train)\n# label1=np.ones((int(598),1))#Value can be changed\n# label2=np.zeros((int(508 ),1))\nlabel1=np.ones((598,1))#Value can be changed\nlabel2=np.zeros((508,1))\nlabel=np.append(label1,label2)\nshu=scale(data_train)\n\n# Split the data into training (80%) and testing (20%) sets\nX_train, X_test, y_train, y_test = train_test_split(shu, label, test_size=0.2, random_state=42)\n\nX_ind_test = xt\n# X_independent = shu.reshape(m1, n1, 1)\n# Xt=np.reshape(test_data,(-1,1,n1))\ny_ind_test = yt\n\n# Scale the independent data using the same scaling method as the training data\ndef scale_mean_var(input_arr, axis=0):\n    mean_ = np.mean(input_arr, axis=0)\n    scale_ = np.std(input_arr, axis=0)\n    output_arr = input_arr - mean_\n    mean_1 = output_arr.mean(axis=0)\n    if not np.allclose(mean_1, 0):\n        output_arr -= mean_1\n    scale_[scale_ == 0.0] = 1.0\n    output_arr /= scale_\n    mean_2 = output_arr.mean(axis=0)\n    if not np.allclose(mean_2, 0):\n        output_arr -= mean_2\n    return output_arr\n\n# Scale the independent test data\nX_ind_test_scaled = scale_mean_var(X_ind_test)\n\n# Load the pre-trained model\ndiscriminator = load_model('/kaggle/working/models/discriminator_model_fold_2.h5')\ngenerator = load_model('/kaggle/working/models/generator_model_fold_2.h5')\n\n# Build the combined model\nz = Input(shape=(X_ind_test_scaled.shape[1],))\nimg = generator(z)\ndiscriminator.trainable = False\nvalid = discriminator(img)\ncombined = Model(z, valid)\ncombined.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\n\n# Evaluate the model on the independent test data\ny_ind_test_categorical = to_categorical(y_ind_test)\ny_score = combined.predict(X_ind_test_scaled)\ny_class = np.argmax(y_score, axis=1)\n\n# Calculate metrics\nTP, FP, FN, TN = confusion_matrix(y_ind_test, y_class).ravel()\nSn = TP / (TP + FN)\nSp = TN / (TN + FP)\nMCC = (TP * TN - FP * FN) / math.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))\nBACC = 0.5 * Sn + 0.5 * Sp\n\n# ROC curve\nfpr, tpr, _ = roc_curve(y_ind_test, y_score[:, 1])\nroc_auc = auc(fpr, tpr)\n\n# PR curve\nprecision, recall, _ = precision_recall_curve(y_ind_test, y_score[:, 1])\naverage_precision = average_precision_score(y_ind_test, y_score[:, 1])\n\n# Output results\nprint(f\"BACC: {BACC:.3f}\")\nprint(f\"Sn: {Sn:.3f}\")\nprint(f\"Sp: {Sp:.3f}\")\nprint(f\"MCC: {MCC:.3f}\")\nprint(f\"AUC: {roc_auc:.3f}\")\nprint(f\"Average Precision: {average_precision:.3f}\")\n\n# Plot ROC curve\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([-0.05, 1.05])\nplt.ylim([-0.05, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve on Independent Data')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n# Plot PR curve\nplt.figure()\nplt.step(recall, precision, where='post', color='b', alpha=0.2, label='Average precision (area = %0.2f)' % average_precision)\nplt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve on Independent Data')\nplt.legend(loc=\"lower right\")\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport sys\nimport math\nimport numpy as np\nimport pandas as pd\nimport statistics\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import scale\nfrom tensorflow.keras.models import load_model\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import Input, Model\n\n# Load datasets\nBiLSTM = pd.read_csv(r'/kaggle/input/qsar-model/all_BiLSTM_train_2D.csv', header=None)\nAttention = pd.read_csv(r'/kaggle/input/qsar-model/all_atten_train_2D.csv', header=None)\ndataset = np.column_stack((BiLSTM, Attention))\n\n# Prepare training data\ndata_train = np.array(dataset)[:1106, :]\nlabel1 = np.ones((598, 1))  # Value can be changed\nlabel2 = np.zeros((508, 1))\nlabel = np.append(label1, label2)\nshu = scale(data_train)\n\n# Split the data into training (80%) and testing (20%) sets\nX_train, X_test, y_train, y_test = train_test_split(shu, label, test_size=0.2, random_state=42)\n\n# Independent test data (Assuming xt and yt are predefined)\nX_ind_test = xt\ny_ind_test = yt\n\n# Scale the independent data using the same scaling method as the training data\ndef scale_mean_var(input_arr, axis=0):\n    mean_ = np.mean(input_arr, axis=0)\n    scale_ = np.std(input_arr, axis=0)\n    output_arr = input_arr - mean_\n    mean_1 = output_arr.mean(axis=0)\n    if not np.allclose(mean_1, 0):\n        output_arr -= mean_1\n    scale_[scale_ == 0.0] = 1.0\n    output_arr /= scale_\n    mean_2 = output_arr.mean(axis=0)\n    if not np.allclose(mean_2, 0):\n        output_arr -= mean_2\n    return output_arr\n\n# Scale the independent test data\nX_ind_test_scaled = scale_mean_var(X_ind_test)\n\n# List of model filenames\nmodel_files = [f'/kaggle/working/models/discriminator_model_fold_{i}.h5' for i in range(1, 10)]\ngenerator_files = [f'/kaggle/working/models/generator_model_fold_{i}.h5' for i in range(1, 10)]\n\n# Loop through the models and evaluate each\nfor i, (discriminator_file, generator_file) in enumerate(zip(model_files, generator_files)):\n    print(f\"Evaluating model {i+1}/{len(model_files)}\")\n\n    # Load the pre-trained models\n    discriminator = load_model(discriminator_file)\n    generator = load_model(generator_file)\n\n    # Build the combined model\n    z = Input(shape=(X_ind_test_scaled.shape[1],))\n    img = generator(z)\n    discriminator.trainable = False\n    valid = discriminator(img)\n    combined = Model(z, valid)\n    combined.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\n\n    # Evaluate the model on the independent test data\n    y_ind_test_categorical = to_categorical(y_ind_test)\n    y_score = combined.predict(X_ind_test_scaled)\n    y_class = np.argmax(y_score, axis=1)\n\n    # Calculate metrics\n    TP, FP, FN, TN = confusion_matrix(y_ind_test, y_class).ravel()\n    Sn = TP / (TP + FN)\n    Sp = TN / (TN + FP)\n    MCC = (TP * TN - FP * FN) / math.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))\n    BACC = 0.5 * Sn + 0.5 * Sp\n\n    # ROC curve\n    fpr, tpr, _ = roc_curve(y_ind_test, y_score[:, 1])\n    roc_auc = auc(fpr, tpr)\n\n    # PR curve\n    precision, recall, _ = precision_recall_curve(y_ind_test, y_score[:, 1])\n    average_precision = average_precision_score(y_ind_test, y_score[:, 1])\n\n    # Output results\n    print(f\"BACC: {BACC:.3f}\")\n    print(f\"Sn: {Sn:.3f}\")\n    print(f\"Sp: {Sp:.3f}\")\n    print(f\"MCC: {MCC:.3f}\")\n    print(f\"AUC: {roc_auc:.3f}\")\n    print(f\"Average Precision: {average_precision:.3f}\")\n\n    # Plot ROC curve\n    plt.figure()\n    lw = 2\n    plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n    plt.xlim([-0.05, 1.05])\n    plt.ylim([-0.05, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title(f'ROC Curve on Independent Data for Model {i+1}')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    # Plot PR curve\n    plt.figure()\n    plt.step(recall, precision, where='post', color='b', alpha=0.2, label='Average precision (area = %0.2f)' % average_precision)\n    plt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title(f'Precision-Recall Curve on Independent Data for Model {i+1}')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-29T17:10:32.712769Z","iopub.execute_input":"2024-05-29T17:10:32.713582Z","iopub.status.idle":"2024-05-29T17:10:59.521422Z","shell.execute_reply.started":"2024-05-29T17:10:32.713545Z","shell.execute_reply":"2024-05-29T17:10:59.520320Z"},"trusted":true},"execution_count":null,"outputs":[]}]}